{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier for Hard Negative Mining Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Run some setup code for this notebook.\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#import lr_scheduler\n",
    "import torch.optim.lr_scheduler\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import operator\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "\n",
    "# This is to make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# So the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def applySIFT(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp = sift.detect(gray, None)\n",
    "    img = cv2.drawKeypoints(gray, kp, None)\n",
    "    return kp\n",
    "\n",
    "def extractSift(input_files):\n",
    "    print (\"extracting Sift features\")\n",
    "    all_features_dict = {}\n",
    "    for i, fname in enumerate(input_files):\n",
    "        features_fname = fname + '.sift'\n",
    "        if exists(features_fname) == False:\n",
    "            print (\"calculating sift features for\", fname)\n",
    "            sift.process_image(fname, features_fname)\n",
    "        print (\"gathering sift features for\", fname,\n",
    "        locs, descriptors = sift.read_features_from_file(features_fname))\n",
    "        print (descriptors.shape)\n",
    "        all_features_dict[fname] = descriptors\n",
    "    return all_features_dict\n",
    "\n",
    "def dict2numpy(dict):\n",
    "    nkeys = len(dict)\n",
    "    array = zeros((nkeys * PRE_ALLOCATION_BUFFER, 128))\n",
    "    pivot = 0\n",
    "    for key in dict.keys():\n",
    "        value = dict[key]\n",
    "        nelements = value.shape[0]\n",
    "        while pivot + nelements > array.shape[0]:\n",
    "            padding = zeros_like(array)\n",
    "            array = vstack((array, padding))\n",
    "        array[pivot:pivot + nelements] = value\n",
    "        pivot += nelements\n",
    "    array = resize(array, (pivot, 128))\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import AdditiveChi2Sampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from feature_aggregation import BagOfWords\n",
    "\n",
    "def sift(img):\n",
    "    sift= cv2.xfeatures2d.SIFT_create()\n",
    "    kp = sift.detect(img, None)\n",
    "\n",
    "def dsift(img, step=5):\n",
    "    keypoints = [\n",
    "        cv2.KeyPoint(x, y, step)\n",
    "        for y in range(0, img.shape[0], step)\n",
    "        for x in range(0, img.shape[1], step)\n",
    "    ]\n",
    "    features = sift().compute(img, keypoints)[1]\n",
    "#     if (features.sum(axis=1).reshape(-1, 1)).any() == 0:\n",
    "    features /= features.sum(axis=1).reshape(-1, 1)\n",
    "    print(features)\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dense SIFT features\n",
    "data_dir_path = './facesDataAll'\n",
    "\n",
    "# Iterate over data.\n",
    "X_train = []\n",
    "y_train = []\n",
    "train_path = os.path.join(data_dir_path, 'train')\n",
    "features = [];\n",
    "for dir in os.listdir(train_path):\n",
    "    if dir == 'attractive':\n",
    "        label = 1\n",
    "    elif dir == 'unattractive':\n",
    "        label = 0\n",
    "    else:\n",
    "        continue\n",
    "    subFolder_path = os.path.join(train_path, dir)\n",
    "    for f in os.listdir(subFolder_path):\n",
    "        if not f.startswith('.'):\n",
    "            image = cv2.imread(os.path.join(subFolder_path, f))\n",
    "#             (image.reshape(64, 64, 1)*255).astype(np.uint8)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "            keypoints = dsift(gray)\n",
    "            features.append(keypoints)\n",
    "#             X_train.append(np.asarray(keypointsSIFT))\n",
    "            y_train.append(label)\n",
    "\n",
    "X_dev = []\n",
    "y_dev = []\n",
    "dev_path = os.path.join(data_dir_path, 'dev')\n",
    "for dir in os.listdir(dev_path):\n",
    "    if dir == 'attractive':\n",
    "        label = 1\n",
    "    elif dir == 'unattractive':\n",
    "        label = 0\n",
    "    else:\n",
    "        continue\n",
    "    subFolder_path = os.path.join(dev_path, dir)\n",
    "    for f in os.listdir(subFolder_path):\n",
    "        if not f.startswith('.'):\n",
    "            image = cv2.imread(os.path.join(subFolder_path, f))\n",
    "#             (image.reshape(64, 64, 1)*255).astype(np.uint8)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "            keypoints = dsift(gray)\n",
    "            features.append(keypoints)\n",
    "#             print(os.path.join(subFolder_path, f))\n",
    "#             print(type(image))\n",
    "#             print(image.shape)\n",
    "            keypointsSIFT = applySIFT(image)\n",
    "            X_dev.append(np.asarray(keypointsSIFT))\n",
    "            y_dev.append(label)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "test_path = os.path.join(data_dir_path, 'test')\n",
    "for dir in os.listdir(test_path):\n",
    "    if dir == 'attractive':\n",
    "        label = 1\n",
    "    elif dir == 'unattractive':\n",
    "        label = 0\n",
    "    else:\n",
    "        continue\n",
    "    subFolder_path = os.path.join(test_path, dir)\n",
    "    for f in os.listdir(subFolder_path):\n",
    "        if not f.startswith('.'):\n",
    "            image = cv2.imread(os.path.join(subFolder_path, f))\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            keypoints = dsift(gray)\n",
    "            features.append(keypoints)\n",
    "            \n",
    "            keypointsSIFT = applySIFT(image)\n",
    "            X_test.append(np.asarray(keypointsSIFT))\n",
    "            y_test.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-85e5616e1da6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfaces_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heathershen/miniconda2/lib/python2.7/site-packages/feature_aggregation/bow.pyc\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mnd\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reshape_local_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clusterer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heathershen/miniconda2/lib/python2.7/site-packages/feature_aggregation/base.pyc\u001b[0m in \u001b[0;36m_reshape_local_features\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X cannot be empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_ordering\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"th\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             arrays = [\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Aggregate those features with bag of words using online training\n",
    "print(features)\n",
    "bow = BagOfWords(100)\n",
    "for i in range(2):\n",
    "    for j in range(0, len(features), 10):\n",
    "        bow.partial_fit(features[j:j+10])\n",
    "faces_bow = bow.transform(features)\n",
    "\n",
    "# Split in training and test set\n",
    "train = np.arange(len(features))\n",
    "np.random.shuffle(train)\n",
    "test = train[200:]\n",
    "train = train[:200]\n",
    "\n",
    "# Train and evaluate\n",
    "svm = Pipeline([(\"chi2\", AdditiveChi2Sampler()), (\"svm\", LinearSVC(C=10))])\n",
    "svm.fit(faces_bow[train], faces.target[train])\n",
    "print(classification_report(faces.target[test], svm.predict(faces_bow[test])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine SIFT Keypoints in all images and use as our data\n",
    "\n",
    "data_dir_path = './facesDataAll'\n",
    "\n",
    "# Iterate over data.\n",
    "X_train = []\n",
    "y_train = []\n",
    "train_path = os.path.join(data_dir_path, 'train')\n",
    "for dir in os.listdir(train_path):\n",
    "    if dir == 'attractive':\n",
    "        label = 1\n",
    "    elif dir == 'unattractive':\n",
    "        label = 0\n",
    "    else:\n",
    "        continue\n",
    "    subFolder_path = os.path.join(train_path, dir)\n",
    "    for f in os.listdir(subFolder_path):\n",
    "        if not f.startswith('.'):\n",
    "            image = cv2.imread(os.path.join(subFolder_path, f))\n",
    "            keypointsSIFT = applySIFT(image)\n",
    "            X_train.append(np.asarray(keypointsSIFT))\n",
    "            y_train.append(label)\n",
    "\n",
    "X_dev = []\n",
    "y_dev = []\n",
    "dev_path = os.path.join(data_dir_path, 'dev')\n",
    "for dir in os.listdir(dev_path):\n",
    "    if dir == 'attractive':\n",
    "        label = 1\n",
    "    elif dir == 'unattractive':\n",
    "        label = 0\n",
    "    else:\n",
    "        continue\n",
    "    subFolder_path = os.path.join(dev_path, dir)\n",
    "    for f in os.listdir(subFolder_path):\n",
    "        if not f.startswith('.'):\n",
    "            image = cv2.imread(os.path.join(subFolder_path, f))\n",
    "#             print(os.path.join(subFolder_path, f))\n",
    "#             print(type(image))\n",
    "#             print(image.shape)\n",
    "            keypointsSIFT = applySIFT(image)\n",
    "            X_dev.append(np.asarray(keypointsSIFT))\n",
    "            y_dev.append(label)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "test_path = os.path.join(data_dir_path, 'test')\n",
    "for dir in os.listdir(test_path):\n",
    "    if dir == 'attractive':\n",
    "        label = 1\n",
    "    elif dir == 'unattractive':\n",
    "        label = 0\n",
    "    else:\n",
    "        continue\n",
    "    subFolder_path = os.path.join(test_path, dir)\n",
    "    for f in os.listdir(subFolder_path):\n",
    "        if not f.startswith('.'):\n",
    "            image = cv2.imread(os.path.join(subFolder_path, f))\n",
    "            keypointsSIFT = applySIFT(image)\n",
    "            X_test.append(np.asarray(keypointsSIFT))\n",
    "            y_test.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_dev = np.array(X_dev)\n",
    "y_dev = np.array(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (948, 1)\n",
      "Train labels shape:  (948, 1)\n",
      "Test data shape:  (183, 1)\n",
      "Test labels shape:  (183, 1)\n",
      "Dev data shape:  (180, 1)\n",
      "Dev labels shape:  (180, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Flatten and convert data into rows as part of out preprocess\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "#reshape y's\n",
    "y_train = np.reshape(y_train, (X_train.shape[0], 1))\n",
    "y_test = np.reshape(y_test, (X_test.shape[0], 1))\n",
    "y_dev = np.reshape(y_dev, (X_dev.shape[0], 1))\n",
    "\n",
    "#Sanity Check\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "\n",
    "\n",
    "print('Dev data shape: ', X_dev.shape)\n",
    "print('Dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (238,) (129,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-45b8616d9c47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Normalizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmean_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mmean_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# X_val -= mean_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heathershen/miniconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2909\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/heathershen/miniconda2/lib/python2.7/site-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (238,) (129,) "
     ]
    }
   ],
   "source": [
    "# Normalizing\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train -= mean_image\n",
    "# X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add bias\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "# X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "# print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n",
    "print(X_train.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "Compute SVM to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from utils.classifiers import LinearSVM\n",
    "# svm = LinearSVM()\n",
    "# tic = time.time()\n",
    "# loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,\n",
    "#                       num_iters=1500, verbose=True)\n",
    "# toc = time.time()\n",
    "# print('That took %fs' % (toc - tic))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "tic = time.time()\n",
    "clf.fit(X_train, y_train.ravel()) \n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))\n",
    "\n",
    "\n",
    "test_acc = clf.score(X_test, y_test.ravel())\n",
    "train_acc = clf.score(X_train, y_train.ravel())\n",
    "print('Train acc: ', train_acc)\n",
    "print('Test acc: ', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
